[
  {
    "objectID": "Take-Home_Ex/Take-Home Ex1.html",
    "href": "Take-Home_Ex/Take-Home Ex1.html",
    "title": "Take-home Exercise 1 - Geospatial Analytics for Social Good",
    "section": "",
    "text": "Water is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n\n\nThis study aims to apply appropriate global and local measures of spatial Association techniques to reveal the spatial patterns of Not Functional water points in Nigeria.\n\n\n\nUsing the appropriate R packages, we will need to:\n\nPrepare the dataset and save it in simple feature data frameformat, as well as derive the proportion of functional and non-functional water point at LGA level\nConduct thematic mapping analysis to examine the spatial distribution of functional and non-functional water point rate at LGA level\nConduct hotspot areas and outliers/clusters maps of functional and non0functional water point rate at LGA level"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home Ex1.html#setup",
    "href": "Take-Home_Ex/Take-Home Ex1.html#setup",
    "title": "Take-home Exercise 1 - Geospatial Analytics for Social Good",
    "section": "2. Setup",
    "text": "2. Setup\n\n2.1 Packages Used\nThe R packages that we will be using for this analysis area:\n\nsf: used for importing, managing, and processing geospatial data\ntidyverse: used for wrangling attribute data\nspdep: used for computing spatial weights, global and local spatial auto-correlation statistics\ntmap: used for creating cartographic quality choropleth map\nfunModeling: used for exploratory data analysis, data preparation and model performance\n\nIn addition, the following tidyverse packages will be used:\n\ntidyr for manipulating and tidying data\ndplyr for wrangling and transforming data\nggplot2 for visualising data\n\n\n\n2.2 Datasets Used\n2 geospatial datasets will be utilized for this study:\n\ngeo_export_338e5689-bd72-4866-bfde-8997933e9897\nWPdx+ dataset from WPdx Global Data Repositories\nnga_admbnda_adm2_osgof_20190417\nNigeria Level-2 Administrative Boundary (also known as Local Government Area) polygon features GIS data from geoBoundaries\n\n\n\n2.3 Launching the packages in R\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, funModeling)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home Ex1.html#data-preparation",
    "href": "Take-Home_Ex/Take-Home Ex1.html#data-preparation",
    "title": "Take-home Exercise 1 - Geospatial Analytics for Social Good",
    "section": "3. Data Preparation",
    "text": "3. Data Preparation\nIn this section, we will bring geospatial data into R environment. The geospatial data is in ESRI shapefile format.\n\n3.1 Import water point shapefile into R environment\nThe code chunk below uses st_read() of sf package to import Nigeria shapefile into R. The imported shapefile will be simple features Object of sf.\n\nwp <- st_read(dsn = \"geodata\",\n              layer = \"geo_export_338e5689-bd72-4866-bfde-8997933e9897\",\n              crs = 4326) %>%\n  filter(clean_coun == \"Nigeria\") %>%\n  select(1:4, 13:15, 23, 36:40)\n\nThe code chunk below uses write_rds() of readr package to save the extracted sf data table (i.e. wp) into an output in rds data format. The output file is called wp_nga.rds and it is saved in geodata sub-folder.\n\nwp_nga <- write_rds(wp, \"geodata/wp_nga.rds\")\n\n\n\n3.2 Import Nigeria LGA boundary data into R environment\nWe are going to import LGA boundary data into R environment using the following code chunk, st_read() of sf package. It is used to import nga_admbnda_adm2_osgof_20190417 shapefile and save the imported geospatial data into simple feature data table.\n\nnga <- st_read(dsn = \"geodata\",\n               layer = \"nga_admbnda_adm2_osgof_20190417\",\n               crs = 4326)\n\n\n\n3.3 Recoding NA values into string\nUse replace_na() to recode all the NA values in status_cle field into the Unknown.\n\nwp_nga <- read_rds(\"geodata/wp_nga.rds\") %>%\n  mutate(status_cle = replace_na(status_cle, \"Unknown\"))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home Ex1.html#exploratory-spatial-data-analysis",
    "href": "Take-Home_Ex/Take-Home Ex1.html#exploratory-spatial-data-analysis",
    "title": "Take-home Exercise 1 - Geospatial Analytics for Social Good",
    "section": "4. Exploratory Spatial Data Analysis",
    "text": "4. Exploratory Spatial Data Analysis\nUse freq() of funModeling package to display the distribution of status_cle field in wp_nga.\n\nfreq(data = wp_nga,\n     input = \"status_cle\")\n\n\n4.1 Extract functional water point data\nIn the code chunk below, filter() of dplyr is used to select functional water points.\n\nwpt_functional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Functional\", \n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nfreq(data=wpt_functional, \n     input = 'status_cle')\n\n\n\n4.2 Extract non-functional water point data\nIn the code chunk below, filter() of dplyr is used to select non-functional water points.\n\nwpt_nonfunctional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Abandoned/Decommissioned\", \n             \"Abandoned\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\",\n             \"Non-Functional due to dry season\"))\n\nfreq(data=wpt_nonfunctional, \n     input = 'status_cle')\n\n\n\n4.3 Extract water point data with Unknown class\nIn the code chunk below, filter() of dplyr is used to select unknown water points.\n\nwpt_unknown <- wp_nga %>%\n  filter(status_cle == \"Unknown\")\n\n\n\n4.4 Performing Point-in-Polygon Count\nThe code chunk below performs 2 operations at one go. Firstly, it uses st_intersects() to identify the various water point types (e.g. total, functional, non-functional and unknown) located inside each LGA boundary. Next, length() of Base R is used to calculate the number of water points that fall within each LGA boundary.\n\nnga_wp <- nga %>% \n  mutate(`total wpt` = lengths(\n    st_intersects(nga, wp_nga))) %>%\n  mutate(`wpt functional` = lengths(\n    st_intersects(nga, wpt_functional))) %>%\n  mutate(`wpt non-functional` = lengths(\n    st_intersects(nga, wpt_nonfunctional))) %>%\n  mutate(`wpt unknown` = lengths(\n    st_intersects(nga, wpt_unknown)))\n\n\n\n4.5 Saving the Analytical Data Table\nThe code chunk below uses mutate() of dplyr package to derive 2 fields namely pct_functional and pct_non-functional. In order to keep the file size small, select() of dplyr is used to retain on the relevant fields.\n\nnga_wp <- nga_wp %>%\n  mutate(pct_functional = `wpt functional`/`total wpt`) %>%\n  mutate(`pct_non-functional` = `wpt non-functional`/`total wpt`) %>%\n  select(3:4, 9:10, 18:23)\n\nThereafter, we will save the sf data table in rds format for subsequent analysis.\n\nwrite_rds(nga_wp, \"geodata/nga_wp.rds\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home Ex1.html#geospatial-visualization-analysis",
    "href": "Take-Home_Ex/Take-Home Ex1.html#geospatial-visualization-analysis",
    "title": "Take-home Exercise 1 - Geospatial Analytics for Social Good",
    "section": "5. Geospatial Visualization & Analysis",
    "text": "5. Geospatial Visualization & Analysis\n\n5.1 Thematic Mapping of Functional and Non-Functional Water Points at LGA level\nIn order to draw a choropleth map, we will use qtm() of tmap package. Small choropleth maps are created with tmap_arrange().\n\nnga_wp <- read_rds(\"geodata/nga_wp.rds\")\ntotal <- qtm(nga_wp, \"total wpt\")\nwp_functional <- qtm(nga_wp, \"wpt functional\")\nwp_nonfunctional <- qtm(nga_wp, \"wpt non-functional\")\nunknown <- qtm(nga_wp, \"wpt unknown\")\n\ntmap_arrange(total, wp_functional, wp_nonfunctional, unknown, asp=1, ncol=2)\n\n\n\n5.2 Global Spatial Autocorrelation\nIn this section, we will compute global spatial autocorrelation statistics and perform spatial complete randomness test for global spatial autocorrelation.\n\n5.2.1 Computing Contiguity Spatial Weights\nIn the code chunk below, poly2nb() of spdep package is used to compute the contiguity weight matrices for the LGA. We will compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(nga_wp, \n                queen=TRUE)\n\nsummary(wm_q)\n\nThe summary report above shows that there are 774 area units in Nigera. The most connected area unit has 14 neighbours. There are 2 area units with only 1 neighbour.\n\n\n5.2.2 Row-standardized weights matrix\nAlternatively, we can assign weights to each neighbouring polygon. In this study, each of the neighbouring polygon will be assigned equal weight (style = “W”). This is accomplished by assigning the fraction 1/(# of neighbors) to each neighboring county then summing the weighted income values.\n\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\n\nset.ZeroPolicyOption(TRUE)\n\nrswm_q\n\n\n\n5.2.3 Performing Moran’s I test\nUsing localmoran() function of spdep, we will compute local Moran’s I. We will compute local indicator values, given a set of standard deviation values and listw objective providing the neighbour weighting information of the polygon associated with standard deviation values.\nThe code chunk below is used to compute local Moran’s I of non-functional waterpoints at the LGA.\n\nmoran.test(nga_wp$`wpt non-functional`, \n           listw = rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\nThe above statistical output illustrates that the null hypothesis i.e. observed spatial pattern of values is equally likely as other spatial pattern can be rejected. There is sufficient evidence to show that regions with higher percentage of non-functional water points are dependent on those at other (neighbouring) locations.\n\n\n5.2.4 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(nga_wp$`wpt non-functional`, listw=rswm_q)\n\nThe above statistical output illustrates that the null hypothesis i.e. observed spatial pattern of values is similar from their immediate neighbours can be rejected. There is sufficient evidence to show that regions with higher percentage of non-functional water points are dissimilar to their (neighbouring) locations.\n\n\n\n5.3 Cluster and Outlier Analysis\n\n5.3.1 Computing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes the local Moran’s I statistic values, given a set of standard deviation and a listw object providing neighbour weighting information for the polygon associated with standard deviation.\nThe code chunk below is used to compute local Moran’s I of non-functional water point at the county level.\n\nfips <- order(nga_wp$ADM2_EN)\nlocalMI <- localmoran(nga_wp$`wpt non-functional`, rswm_q)\nhead(localMI)\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names = nga_wp$ADM2_PCODE[fips]),\n  check.names=FALSE)\n\n\n\n5.3.2 Mapping both local Moran’s I values and p-values\nThe code chunk below is meant to append the local Moran’s I dataframe (i.e. localMI) onto Nigera Spatial Polygon Data Frame.\n\nnga_wp.localMI <- cbind(nga_wp,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nUsing choropleth mapping functions of tmap package, we will plot the local Moran’s I and p-values with the code chunk below.\n\nlocalMI.map <- tm_shape(nga_wp.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map <- tm_shape(nga_wp.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\n\n\n5.3.3 Creating LISA map classes\nThe code chunk below show the steps to prepare a LISA cluster map.\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. wpt_nonfunctional) and centers the spatially lagged variable around its mean.\n\nnga_wp$lag_nonfunctional <- lag.listw(rswm_q, nga_wp$`wpt non-functional`)\n\nDV <- nga_wp$lag_nonfunctional - mean(nga_wp$lag_nonfunctional)     \n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I <- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif <- 0.05 \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV <0 & LM_I>0] <- 1\nquadrant[DV >0 & LM_I<0] <- 2\nquadrant[DV <0 & LM_I<0] <- 3  \nquadrant[DV >0 & LM_I>0] <- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]>signif] <- 0\n\n\n\n5.3.4 Plotting LISA map\nUsing the code chunk below, we will build the LISA map. For effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\n\nwpt_nonfunctional <- qtm(nga_wp, \"wpt_non-functional\")\n\nnga_wp.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap <- tm_shape(nga_wp.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\nLISAmap\n\n\n\n\n5.4 Hot and Cold Spot Analysis\nIn order to detect spatial anomalies, we will use Getis and Ord’s G-Statistics. We will look at neighbours within a defined proximity to identify where either high or low values cluster spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\n\n5.4.1 Deriving the centroid\nThe code chunk below allows to get the longitude and latitude, which is the 1st value and 2nd value in each centroid respectively.\n\nlongitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[1]])\n\nlatitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[2]])\n\nNext we will use cbind to put the longitude and latitude into the same object.\n\ncoords <- cbind(longitude, latitude)\nhead(coords)\n\n\n\n5.4.2 Computing cut-off distance\nThe code chunk below uses dnearneigh() of spdep package to derive distance-based weight matrix.\n\nk1 <- knn2nb(knearneigh(coords))\n\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\nThe summary report shows that the largest first nearest neighbour distance is 71.661 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n5.4.3 Computing fixed distance weight matrix\nThe chunk below computes the distance weight matrix by using dnearneigh() of spdep package.\nNext, we will use str() to display the content of wm_d72 weight matrix.\n\nwm_d72 <- dnearneigh(coords, 0, 72, longlat = TRUE)\nwm_d72\n\nnb2listw() is used to convert the nb object into spatial weights object.\n\nwm72_lw <- nb2listw(wm_d72, style = 'B')\nsummary(wm72_lw)\n\n\n\n5.4.4 Computing Gi statistics using fixed distance\n\nfips <- order(nga_wp$ADM2_EN)\ngi.fixed <- localG(nga_wp$'wpt non-functional', wm72_lw)\ngi.fixed\n\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding nga_wp sf data frame by using the code chunk below.\n\nnga_wp.gi <- cbind(nga_wp, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\n\n5.4.5 Mapping Gi values using fixed distance\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\nwpt_nonfunctional <- qtm(nga_wp, \"wpt_non-functional\")\n\nGimap <-tm_shape(nga_wp.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n1f2a596 (Commit)"
  }
]